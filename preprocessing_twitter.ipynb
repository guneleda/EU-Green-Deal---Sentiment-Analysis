{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "executionInfo": {
     "elapsed": 259,
     "status": "ok",
     "timestamp": 1683534608560,
     "user": {
      "displayName": "Eda Günel",
      "userId": "11013733903177303205"
     },
     "user_tz": -120
    },
    "id": "yMo8yosqqAP4",
    "outputId": "6727dc47-274c-4054-b3ec-9c5016663aa4"
   },
   "source": [
    "# Pre-processing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 177723,
     "status": "ok",
     "timestamp": 1683752741787,
     "user": {
      "displayName": "Eda Günel",
      "userId": "11013733903177303205"
     },
     "user_tz": -120
    },
    "id": "H9sMQbSzbFuR",
    "outputId": "c00222ef-a1b6-4760-846d-c9b5f2389693"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 584,
     "status": "ok",
     "timestamp": 1683752749323,
     "user": {
      "displayName": "Eda Günel",
      "userId": "11013733903177303205"
     },
     "user_tz": -120
    },
    "id": "ZjszDFjlcaaS",
    "outputId": "d52c2e2f-1cc1-4ff6-8676-1ca3630edcf4"
   },
   "outputs": [],
   "source": [
    "%cd /content/drive/My Drive/Tilburg DSS/Thesis/Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nJ9XFjq2qNzy"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install anonymizedf\n",
    "from anonymizedf.anonymizedf import anonymize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "atft1056qP6J"
   },
   "outputs": [],
   "source": [
    "df=pd.read_excel(\"EUGreenDeal.xlsx\") #Collected Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yKGAjOWFfTn6"
   },
   "outputs": [],
   "source": [
    "df=df[(df.language==\"en\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J5Vm7SiUrf-m"
   },
   "outputs": [],
   "source": [
    "import re \n",
    "def clean_tweet(r):\n",
    "    r = re.sub(r'http\\S+', '', r)\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KFBdp7qSriL4"
   },
   "outputs": [],
   "source": [
    "tweet_list = df.tweet.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BMbeOPAnrtUf"
   },
   "outputs": [],
   "source": [
    "cleaned_url = [clean_tweet(tw) for tw in tweet_list]\n",
    "df[\"cleaned_url\"] =cleaned_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XVzFhBTyryU4"
   },
   "outputs": [],
   "source": [
    "df.drop_duplicates(subset='cleaned_url',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RwtembPCr06o"
   },
   "outputs": [],
   "source": [
    "columns=['id', 'date',\n",
    "       'tweet', 'language', 'hashtags', 'user_id',\n",
    "       'username','cleaned_url']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BYeajD_fr29f"
   },
   "outputs": [],
   "source": [
    "df1=df[columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u-oTOCnZhZyX"
   },
   "outputs": [],
   "source": [
    "df1 = df1.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WsI2Pw0Lr4vr"
   },
   "outputs": [],
   "source": [
    "df2=df1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Anonimizing the Users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nOrT3lhQu6Il"
   },
   "outputs": [],
   "source": [
    "df_an = anonymize(df2)\n",
    "anon_df = (\n",
    "    df_an\n",
    "    .fake_categories(\"username\", chaining=True)\n",
    "    .show_data_frame()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5jJ77lU2vWe2"
   },
   "outputs": [],
   "source": [
    "df2=anon_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User sampling & Daily Number of Tweet Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 6278,
     "status": "ok",
     "timestamp": 1683759052073,
     "user": {
      "displayName": "Eda Günel",
      "userId": "11013733903177303205"
     },
     "user_tz": -120
    },
    "id": "qV1HDOZusjbg",
    "outputId": "630414f3-99d4-47d8-a1b4-44b6341b050f"
   },
   "outputs": [],
   "source": [
    "df2[\"Fake_username\"].value_counts()\n",
    "plt.title(\"# of tweets per users\")\n",
    "plt.xlabel(\"users\")\n",
    "plt.ylabel(\"# of tweets (%)\")\n",
    "plt.savefig('Users_before.png', bbox_inches='tight', dpi=300, format='png')\n",
    "d=df2[\"Fake_username\"].value_counts()\n",
    "e=round(d/sum(df2[\"Fake_username\"].value_counts())*100,2)\n",
    "e[:20].plot(kind=\"bar\", color=\"darkblue\")\n",
    "plt.show()\n",
    "df2[(df2.Fake_username==\"username 1\")]\n",
    "df2\n",
    "df3=df2[(df2.Fake_username==\"username 1\")].sample(n=500, random_state=1)\n",
    "df4=df2[(df2.Fake_username!=\"username 1\")]\n",
    "df4[\"Fake_username\"].value_counts()\n",
    "frames=[df3,df4]\n",
    "df5=pd.concat(frames)\n",
    "b=df5[\"Fake_username\"].value_counts()\n",
    "b\n",
    "c=round(b/sum(df5[\"Fake_username\"].value_counts())*100,2)\n",
    "\n",
    "plt.title(\"# of tweets per users\")\n",
    "plt.xlabel(\"users\")\n",
    "plt.ylabel(\"# of tweets (%)\")\n",
    "c[:20].plot(kind=\"bar\", color=\"darkblue\")\n",
    "plt.ylim(0, 4)\n",
    "plt.show()\n",
    "plt.savefig('Users_after.png', bbox_inches='tight', dpi=300, format='png')\n",
    "\n",
    "df5.nunique()\n",
    "df5['Dates'] = pd.to_datetime(df5['date']).dt.date\n",
    "df5['Time'] = pd.to_datetime(df5['date']).dt.time\n",
    "\n",
    "\n",
    "df2['Dates'] = pd.to_datetime(df2['date']).dt.date\n",
    "df2['Time'] = pd.to_datetime(df2['date']).dt.time\n",
    "\n",
    "# reference: https://gist.github.com/gdsaxton/3d4934c61a435768e687d01aa3f46b4a\n",
    "def tweets_per_day(df):\n",
    "    df['Dates'] = pd.to_datetime(df['Dates'])\n",
    "    return df[['tweet']].groupby(df['Dates'].dt.date).count()\n",
    "  \n",
    "\n",
    "Tweets_count=tweets_per_day(df2)\n",
    "Tweets_count.to_excel(\"daily_tweets.xlsx\")\n",
    "from matplotlib import pyplot as plt, dates as mdates\n",
    "plt.figure(figsize=(12,6))\n",
    "daily_plot = Tweets_count['tweet'].plot(kind='line', lw=1, alpha=0.75, legend=True, x_compat=True)\n",
    "daily_plot.set_xlabel('Date', weight='bold', labelpad=15)   \n",
    "daily_plot.set_ylabel('# Tweets', weight='bold', labelpad=15) \n",
    "plt.xticks(fontsize = 10, rotation = -40, ha =\"left\")  \n",
    "plt.yticks(fontsize = 10)                              \n",
    "\n",
    "daily_plot.legend_ = None\n",
    "daily_plot.tick_params(axis='x', pad=5)\n",
    "\n",
    "Tweets_count=tweets_per_day(df5)\n",
    "Tweets_count.to_excel(\"daily_tweets.xlsx\")\n",
    "from matplotlib import pyplot as plt, dates as mdates\n",
    "plt.figure(figsize=(12,6))\n",
    "daily_plot = Tweets_count['tweet'].plot(kind='line', lw=1, alpha=0.75, legend=True, x_compat=True)\n",
    "daily_plot.set_xlabel('Date', weight='bold', labelpad=15)   \n",
    "daily_plot.set_ylabel('# Tweets', weight='bold', labelpad=15) \n",
    "plt.xticks(fontsize = 10, rotation = -40, ha =\"left\") \n",
    "plt.yticks(fontsize = 10)                           \n",
    "\n",
    "\n",
    "daily_plot.legend_ = None\n",
    "daily_plot.tick_params(axis='x', pad=5)\n",
    "plt.savefig('daily counts.png', bbox_inches='tight', dpi=300, format='png') \n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HkqrQ1oEtGhj"
   },
   "outputs": [],
   "source": [
    "df5['tweet_1'] = df5.cleaned_url.str.lower()\n",
    "df5\n",
    "df5['tweet_2']=df5.tweet_1.apply(lambda x: re.sub(r\"www\\.[a-z]?\\.?(com)+|[a-z]+\\.(com)\", '', x))\n",
    "df5\n",
    "df5['tweet_3']=df5.tweet_2.apply(lambda x: re.sub(r'{link}', '', x))\n",
    "df5['tweet_3']=df5.tweet_3.apply(lambda x: re.sub(r\"\\[video\\]\", '', x))\n",
    "\n",
    "\n",
    "# reference: https://stackoverflow.com/questions/19790188/expanding-english-language-contractions-in-python/47091490#47091490\n",
    "\n",
    "def decontracted(phrase):\n",
    "  \n",
    "    phrase = re.sub(r\"won't\", \"will not\", phrase)\n",
    "    phrase = re.sub(r\"can't\", \"cannot\", phrase)\n",
    " \n",
    "    phrase = re.sub(r\"n't\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"'re\", \" are\", phrase)\n",
    "    phrase = re.sub(r\"'s\", \" is\", phrase)\n",
    "    phrase = re.sub(r\"'d\", \" would\", phrase)\n",
    "    phrase = re.sub(r\"'ll\", \" will\", phrase)\n",
    "    phrase = re.sub(r\"'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"'ve\", \" have\", phrase)\n",
    "    phrase = re.sub(r\"'m\", \" am\", phrase)\n",
    "    phrase = re.sub(r\"'em\", \" them\", phrase)\n",
    "    return phrase\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 2759,
     "status": "ok",
     "timestamp": 1683755125842,
     "user": {
      "displayName": "Eda Günel",
      "userId": "11013733903177303205"
     },
     "user_tz": -120
    },
    "id": "sGCFIdabGuj6",
    "outputId": "7d0e7eb6-1043-483c-9e4d-49e598c51689",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df5['tweet_4'] = df5.tweet_3.apply(lambda x: decontracted(x))\n",
    "df5['tweet_5'] = df5.tweet_4.apply(lambda x: re.sub(r'&[a-z]+;', '', x)) \n",
    "\n",
    "df5['tweet_6'] = df5.tweet_5.apply(lambda x: re.sub(r\"@[A-Za-z0-9_]+\",\"\", x))\n",
    "df5['tweet_7'] = df5.tweet_6.apply(lambda x: re.sub(r\"#[A-Za-z0-9_]+\",\"\", x))\n",
    "df5['tweet_8'] = df5.tweet_7.apply(lambda x: re.sub(r\"[^A-Za-z0-9\\s]+\", \"\", x))\n",
    "\n",
    "df5['tweet_9'] = df5.tweet_8.apply(lambda x: re.sub(r\"\\b[0-9]+\\b\\s*\", \"\", x))\n",
    "df5['tweet_9'] = df5.tweet_9.apply(lambda x: re.sub(r'\\w*\\d\\w*', '', x))\n",
    "\n",
    "df5['tweet_10'] = df5.tweet_9.apply(lambda x:re.sub(r'(.)\\1{3,}',r'\\1', x)) #Remove repeated characters\n",
    "df5['tweet_11'] = df5.tweet_10.apply(lambda x: re.sub(r' +',' ',x).strip())\n",
    "\n",
    "def tokenization(text):\n",
    "    text = re.split('\\W+', text)\n",
    "    return text\n",
    "\n",
    "df5['Tweet_tokenized'] = df5['tweet_11'].apply(lambda x: tokenization(x))\n",
    "df5=df5.reset_index(drop=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 19557,
     "status": "ok",
     "timestamp": 1680900376332,
     "user": {
      "displayName": "Eda Günel",
      "userId": "11013733903177303205"
     },
     "user_tz": -120
    },
    "id": "CMunDbNFs5cQ",
    "outputId": "6d1499c1-2ad3-4d58-b1f2-bf46fb84b5e7"
   },
   "outputs": [],
   "source": [
    "# reference: https://github.com/nltk/nltk\n",
    "!pip install nltk\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Spell Checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7259,
     "status": "ok",
     "timestamp": 1680900383573,
     "user": {
      "displayName": "Eda Günel",
      "userId": "11013733903177303205"
     },
     "user_tz": -120
    },
    "id": "88Lp_zDKt9DM",
    "outputId": "dcce4790-8ea4-4dd5-f7df-9c8128641957"
   },
   "outputs": [],
   "source": [
    "!pip install pyspellchecker\n",
    "from spellchecker import SpellChecker\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5860501,
     "status": "ok",
     "timestamp": 1680906377655,
     "user": {
      "displayName": "Eda Günel",
      "userId": "11013733903177303205"
     },
     "user_tz": -120
    },
    "id": "pdL3EDkeIg51",
    "outputId": "ff2d78f9-3a4b-43b2-fbf4-829d05b63766"
   },
   "outputs": [],
   "source": [
    "# reference: https://gist.github.com/ghadj/507e53effcf7fa9e873b3ed485723527\n",
    "\n",
    "spell = SpellChecker()\n",
    "spell.word_frequency.load_words(['url']) \n",
    " \n",
    "a=[]\n",
    "for i in range(0,len(df5.Tweet_tokenized)):\n",
    "  processed_tweet=[]\n",
    "  for word in df5.Tweet_tokenized[i]:\n",
    "        # Replaced misspelled with the one most likely answer\n",
    "        processed_tweet.append(spell.correction(word) if word in spell.unknown(df5.Tweet_tokenized[i]) else word)\n",
    "  a.append(\" \".join(map(str,processed_tweet)))\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R1EEaeqSUl7r"
   },
   "outputs": [],
   "source": [
    "df_processed=pd.DataFrame(a, columns=[\"spelled_checked\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JPshuQRuUokW"
   },
   "outputs": [],
   "source": [
    "df_processed.spelled_checked=df_processed.spelled_checked.str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eYyepkUrUt8F"
   },
   "outputs": [],
   "source": [
    "frames2=[df5,df_processed]\n",
    "df6=pd.concat(frames2, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fBkfPLFJuhzO"
   },
   "outputs": [],
   "source": [
    "df6.to_pickle(\"df6_.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TigKBcln22Ly"
   },
   "outputs": [],
   "source": [
    "def short_words(text):\n",
    "    # remove words between 1 and 2\n",
    "    shortword = re.compile(r'\\W*\\b(?!no)\\w{1,2}\\b')\n",
    "    return shortword.sub('', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JqCMbGYZ3uAV"
   },
   "outputs": [],
   "source": [
    "df6[\"wo_shortwords\"]=df6.spelled_checked.apply(lambda x: short_words(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FAnvKQMGdZH1"
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "additional  = [\"green\", \"europe\", \"european\", \"via\", \"moi\", \"none\"]\n",
    "swords = set().union(stopwords.words('english'),additional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O6n6akHDddaJ"
   },
   "outputs": [],
   "source": [
    "swords.remove(\"not\") #we exclude not, but, no from the stopwords corpus since removing not from the text will change the context of the text\n",
    "swords.remove(\"but\")\n",
    "swords.remove(\"no\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LaoFB8QVdgak"
   },
   "outputs": [],
   "source": [
    "df6[\"wo_stopwords_2\"] = df6.wo_shortwords.apply(lambda x: [i for i in x.split() if not i in swords])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatizing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "29T1LSogGv4T"
   },
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "wordnet_lem = WordNetLemmatizer()\n",
    "\n",
    "def lemm(text):\n",
    "    text = [wordnet_lem.lemmatize(word) for word in text]\n",
    "    return text\n",
    "\n",
    "df6['tweet_lem'] = df6.wo_stopwords_2.apply(lambda x: lemm(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3H9fRJiTGDY7"
   },
   "outputs": [],
   "source": [
    "df6.to_pickle(\"df6_processed.pkl\") #save the DataFrame as a pickle file:"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPNwVpYdtAxVH04CimH/r3a",
   "mount_file_id": "1EYVex6wilO85D-LgPpk_m_-UKo_u6yOe",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
